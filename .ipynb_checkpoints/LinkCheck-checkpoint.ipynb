{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, sys\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to attempt to retrieve\n",
    "def get_attempt(base_url, get_params=None, attempts=10):\n",
    "    for attempt in range(attempts):\n",
    "        time.sleep(0.5)\n",
    "        try:\n",
    "            response = requests.get(base_url, params=get_params, allow_redirects=True)\n",
    "            break\n",
    "        except:\n",
    "            if attempt == attempts - 1:\n",
    "                response = \"FAILED\"\n",
    "            continue\n",
    "    if response == \"FAILED\":\n",
    "        print(f\"Could not search {base_url}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive function to check page for targets and crawl unseen links\n",
    "def check_page(href, done_hrefs, targets, hit_hrefs):\n",
    "    done_hrefs.append(href)\n",
    "    req = get_attempt(done_hrefs[0] + href)\n",
    "    if req == \"FAILED\":\n",
    "        return\n",
    "    html = req.content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    anchors = soup.find_all(\"a\")\n",
    "    for item in anchors:\n",
    "        if 'href' in item.attrs.keys():\n",
    "            if item['href'] in targets: \n",
    "                print(\"Hit!\", href, item)\n",
    "                hit_hrefs.append([href, item])\n",
    "            elif (item['href'] not in done_hrefs) and (item['href'] not in done_hrefs) and (item['href'][:4] != \"http\") and (item['href'].find('?') == -1):\n",
    "                check_page(item['href'], done_hrefs, targets, hit_hrefs)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial inputs - root and target(s) supplied by user\n",
    "root_url = input(\"Please enter a root url: \")\n",
    "done_hrefs = [root_url, '/']\n",
    "hit_hrefs = []\n",
    "\n",
    "targets = []\n",
    "while True:\n",
    "    new_target = input(\"Please enter a target url or press enter: \")\n",
    "    if new_target != \"\":\n",
    "        targets.append(new_target)\n",
    "        if targets[-1][-1] == \"/\":\n",
    "            targets.append(new_target[:-1])\n",
    "        else:\n",
    "            targets.append(new_target + \"/\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to access user-supplied root url and generate initial link list to work from\n",
    "r = get_attempt(root_url)\n",
    "if r == \"FAILED\":\n",
    "    print(\"Failed to access root url\")\n",
    "    sys.exit()\n",
    "root_html = r.content\n",
    "root_soup = BeautifulSoup(root_html, 'html.parser')\n",
    "home_links = root_soup.find_all('a')\n",
    "\n",
    "home_hrefs = []\n",
    "for link in home_links:\n",
    "    if ('href' in link.attrs.keys()) and (link['href'] not in home_hrefs) and (link['href'].find('?') == -1) and (link['href'][:4] != \"http\"):\n",
    "        home_hrefs.append(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over initial link list and recurse to crawl through site - pages with instances of targets are printed\n",
    "for home_href in home_hrefs:\n",
    "    print(f\"Now processing {home_hrefs.index(home_href)+1} of {len(home_hrefs)}, {home_href}\")\n",
    "    check_page(home_href, done_hrefs, targets, hit_hrefs)\n",
    "        \n",
    "print(hit_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of pages with instances of targets\n",
    "print(len(hit_hrefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output\n",
    "for hit in hit_hrefs:\n",
    "    print(f\"{root_url + hit[0]}, {hit[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
